{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import Food101\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import os\n",
    "import shutil\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "def test_acc(net: nn.Module, test_loader: DataLoader):\n",
    "\n",
    "\tnet.to(device)\n",
    "\tnet.eval()\n",
    "\t\n",
    "\ttotal = 0\n",
    "\tcorrect = 0\n",
    "\n",
    "\tfor images, labels in test_loader:\n",
    "\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\ttotal += labels.size(0)\n",
    "\n",
    "\t\toutputs = net(images)\n",
    "\t\t_, predicted = torch.max(outputs, 1)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "\treturn correct / total * 100\n",
    "\n",
    "def train_fn(epochs: int, train_loader: DataLoader, test_loader: DataLoader,\n",
    "             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer, train_dataset_length):\n",
    "\n",
    "\tlosses = []\n",
    "\taccuracies = []\n",
    "\n",
    "\tnet.to(device)\n",
    "\n",
    "\tfor e in range(epochs):\n",
    "\t\tnet.train()\n",
    "\t\trunning_loss = 0.0\n",
    "\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages, labels = images.to(device), labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = net(images)\n",
    "\t\t\tloss = loss_fn(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\trunning_loss += loss.item() * images.size(0)\n",
    "\n",
    "\t\tepoch_loss = running_loss / train_dataset_length\n",
    "\n",
    "\t\tprint(f\"Loss-ul la finalul epocii {e + 1}: {epoch_loss}\")\n",
    "\n",
    "\t\tacc = test_acc(net, test_loader)\n",
    "\t\tprint(f\"Acuratetea la finalul epocii {e + 1} este {acc:.2f}%\")\n",
    "\n",
    "\t\tlosses.append(epoch_loss)\n",
    "\t\taccuracies.append(acc)\n",
    "\n",
    "\t\t# torch.save(net.state_dict(), f'saved_models/leakyrelu_epoch{e + 1}.pkl')\n",
    "\n",
    "\tloss_graph(losses)\n",
    "\taccuracy_graph(accuracies)\n",
    "\n",
    "def loss_graph(losses):\n",
    "    plt.plot(losses)\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "def accuracy_graph(accuracies):\n",
    "    plt.plot(accuracies)\n",
    "\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 105)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dir = \"C:/Users/Vlad Talpiga.VLR_PROJAMZ/OneDrive - Valrom Industrie SRL/Desktop/Facultate/IAVA/Proiect/Food Classifier Final/datasets/data_105/splits/train\"\n",
    "val_dir = \"C:/Users/Vlad Talpiga.VLR_PROJAMZ/OneDrive - Valrom Industrie SRL/Desktop/Facultate/IAVA/Proiect/Food Classifier Final/datasets/data_105/splits/test\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['apple_pie',\n",
       " 'baby_back_ribs',\n",
       " 'baklava',\n",
       " 'beef_carpaccio',\n",
       " 'beef_tartare',\n",
       " 'beet_salad',\n",
       " 'beignets',\n",
       " 'bibimbap',\n",
       " 'bread_pudding',\n",
       " 'breakfast_burrito',\n",
       " 'bruschetta',\n",
       " 'caesar_salad',\n",
       " 'cannoli',\n",
       " 'caprese_salad',\n",
       " 'carrot_cake',\n",
       " 'ceviche',\n",
       " 'cheese_plate',\n",
       " 'cheesecake',\n",
       " 'chicken_curry',\n",
       " 'chicken_quesadilla',\n",
       " 'chicken_wings',\n",
       " 'chocolate_cake',\n",
       " 'chocolate_mousse',\n",
       " 'churros',\n",
       " 'ciorba_de_burta',\n",
       " 'clam_chowder',\n",
       " 'club_sandwich',\n",
       " 'cozonac',\n",
       " 'crab_cakes',\n",
       " 'creme_brulee',\n",
       " 'croque_madame',\n",
       " 'cup_cakes',\n",
       " 'deviled_eggs',\n",
       " 'donuts',\n",
       " 'dumplings',\n",
       " 'edamame',\n",
       " 'eggs_benedict',\n",
       " 'escargots',\n",
       " 'falafel',\n",
       " 'filet_mignon',\n",
       " 'fish_and_chips',\n",
       " 'foie_gras',\n",
       " 'french_fries',\n",
       " 'french_onion_soup',\n",
       " 'french_toast',\n",
       " 'fried_calamari',\n",
       " 'fried_rice',\n",
       " 'frozen_yogurt',\n",
       " 'garlic_bread',\n",
       " 'gnocchi',\n",
       " 'greek_salad',\n",
       " 'grilled_cheese_sandwich',\n",
       " 'grilled_salmon',\n",
       " 'guacamole',\n",
       " 'gyoza',\n",
       " 'hamburger',\n",
       " 'hot_and_sour_soup',\n",
       " 'hot_dog',\n",
       " 'huevos_rancheros',\n",
       " 'hummus',\n",
       " 'ice_cream',\n",
       " 'lasagna',\n",
       " 'lobster_bisque',\n",
       " 'lobster_roll_sandwich',\n",
       " 'macaroni_and_cheese',\n",
       " 'macarons',\n",
       " 'miso_soup',\n",
       " 'mussels',\n",
       " 'nachos',\n",
       " 'omelette',\n",
       " 'onion_rings',\n",
       " 'oysters',\n",
       " 'pad_thai',\n",
       " 'paella',\n",
       " 'pancakes',\n",
       " 'panna_cotta',\n",
       " 'papanasi',\n",
       " 'peking_duck',\n",
       " 'pho',\n",
       " 'pizza',\n",
       " 'pork_chop',\n",
       " 'poutine',\n",
       " 'prime_rib',\n",
       " 'pulled_pork_sandwich',\n",
       " 'ramen',\n",
       " 'ravioli',\n",
       " 'red_velvet_cake',\n",
       " 'risotto',\n",
       " 'samosa',\n",
       " 'sarmale',\n",
       " 'sashimi',\n",
       " 'scallops',\n",
       " 'seaweed_salad',\n",
       " 'shrimp_and_grits',\n",
       " 'spaghetti_bolognese',\n",
       " 'spaghetti_carbonara',\n",
       " 'spring_rolls',\n",
       " 'steak',\n",
       " 'strawberry_shortcake',\n",
       " 'sushi',\n",
       " 'tacos',\n",
       " 'takoyaki',\n",
       " 'tiramisu',\n",
       " 'tuna_tartare',\n",
       " 'waffles']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset.classes))\n",
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Loss-ul la finalul epocii 1: 2.257598648510282\n",
      "Acuratetea la finalul epocii 1 este 54.93%\n",
      "Loss-ul la finalul epocii 2: 1.759920180638631\n",
      "Acuratetea la finalul epocii 2 este 58.61%\n",
      "Loss-ul la finalul epocii 3: 1.6401317129135131\n",
      "Acuratetea la finalul epocii 3 este 59.58%\n",
      "Loss-ul la finalul epocii 4: 1.574181779331631\n",
      "Acuratetea la finalul epocii 4 este 61.62%\n",
      "Loss-ul la finalul epocii 5: 1.5163431432966201\n",
      "Acuratetea la finalul epocii 5 este 62.88%\n",
      "Loss-ul la finalul epocii 6: 1.4650947317093137\n",
      "Acuratetea la finalul epocii 6 este 63.16%\n",
      "Loss-ul la finalul epocii 7: 1.4277936111480471\n",
      "Acuratetea la finalul epocii 7 este 63.52%\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.95, 0.999), weight_decay=0.0001)\n",
    " \n",
    "epochs = 10\n",
    "start = time.time()\n",
    "print('Training started')\n",
    "\n",
    "train_fn(epochs, train_loader, test_loader, model, loss_fn, optimizer, len(train_dataset))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Duration of training: {(end - start) / 60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epochs: int, train_loader: DataLoader, test_loader: DataLoader,\n",
    "             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer, train_dataset_length):\n",
    "\n",
    "\tlosses = []\n",
    "\taccuracies = []\n",
    "\n",
    "\tnet.to(device)\n",
    "\n",
    "\tfor e in range(epochs):\n",
    "\t\tnet.train()\n",
    "\t\trunning_loss = 0.0\n",
    "\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages, labels = images.to(device), labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = net(images)\n",
    "\t\t\tloss = loss_fn(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\trunning_loss += loss.item() * images.size(0)\n",
    "\n",
    "\t\tepoch_loss = running_loss / train_dataset_length\n",
    "\n",
    "\t\tprint(f\"Loss-ul la finalul epocii {e + 1}: {epoch_loss}\")\n",
    "\n",
    "\t\tacc = test_acc(net, test_loader)\n",
    "\t\tprint(f\"Acuratetea la finalul epocii {e + 1} este {acc:.2f}%\")\n",
    "\n",
    "\t\tlosses.append(epoch_loss)\n",
    "\t\taccuracies.append(acc)\n",
    "\n",
    "\t\ttorch.save(net.state_dict(), f'saved_models/relu_epoch{e + 1}.pkl')\n",
    "\n",
    "\tloss_graph(losses)\n",
    "\taccuracy_graph(accuracies)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            resnet,\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 105)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.95, 0.999), weight_decay=0.0001)\n",
    " \n",
    "epochs = 10\n",
    "start = time.time()\n",
    "print('Training started')\n",
    "\n",
    "train_fn(epochs, train_loader, test_loader, model, loss_fn, optimizer, len(train_dataset))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Duration of training: {(end - start) / 60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epochs: int, train_loader: DataLoader, test_loader: DataLoader,\n",
    "             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer, train_dataset_length):\n",
    "\n",
    "\tlosses = []\n",
    "\taccuracies = []\n",
    "\n",
    "\tnet.to(device)\n",
    "\n",
    "\tfor e in range(epochs):\n",
    "\t\tnet.train()\n",
    "\t\trunning_loss = 0.0\n",
    "\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages, labels = images.to(device), labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = net(images)\n",
    "\t\t\tloss = loss_fn(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\trunning_loss += loss.item() * images.size(0)\n",
    "\n",
    "\t\tepoch_loss = running_loss / train_dataset_length\n",
    "\n",
    "\t\tprint(f\"Loss-ul la finalul epocii {e + 1}: {epoch_loss}\")\n",
    "\n",
    "\t\tacc = test_acc(net, test_loader)\n",
    "\t\tprint(f\"Acuratetea la finalul epocii {e + 1} este {acc:.2f}%\")\n",
    "\n",
    "\t\tlosses.append(epoch_loss)\n",
    "\t\taccuracies.append(acc)\n",
    "\n",
    "\t\ttorch.save(net.state_dict(), f'saved_models/sigmoid_epoch{e + 1}.pkl')\n",
    "\n",
    "\tloss_graph(losses)\n",
    "\taccuracy_graph(accuracies)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            resnet,\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 105)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.95, 0.999), weight_decay=0.0001)\n",
    " \n",
    "epochs = 10\n",
    "start = time.time()\n",
    "print('Training started')\n",
    "\n",
    "train_fn(epochs, train_loader, test_loader, model, loss_fn, optimizer, len(train_dataset))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Duration of training: {(end - start) / 60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epochs: int, train_loader: DataLoader, test_loader: DataLoader,\n",
    "             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer, train_dataset_length):\n",
    "\n",
    "\tlosses = []\n",
    "\taccuracies = []\n",
    "\n",
    "\tnet.to(device)\n",
    "\n",
    "\tfor e in range(epochs):\n",
    "\t\tnet.train()\n",
    "\t\trunning_loss = 0.0\n",
    "\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages, labels = images.to(device), labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = net(images)\n",
    "\t\t\tloss = loss_fn(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\trunning_loss += loss.item() * images.size(0)\n",
    "\n",
    "\t\tepoch_loss = running_loss / train_dataset_length\n",
    "\n",
    "\t\tprint(f\"Loss-ul la finalul epocii {e + 1}: {epoch_loss}\")\n",
    "\n",
    "\t\tacc = test_acc(net, test_loader)\n",
    "\t\tprint(f\"Acuratetea la finalul epocii {e + 1} este {acc:.2f}%\")\n",
    "\n",
    "\t\tlosses.append(epoch_loss)\n",
    "\t\taccuracies.append(acc)\n",
    "\n",
    "\t\ttorch.save(net.state_dict(), f'saved_models/elu_epoch{e + 1}.pkl')\n",
    "\n",
    "\tloss_graph(losses)\n",
    "\taccuracy_graph(accuracies)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            resnet,\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 105)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.95, 0.999), weight_decay=0.0001)\n",
    " \n",
    "epochs = 10\n",
    "start = time.time()\n",
    "print('Training started')\n",
    "\n",
    "train_fn(epochs, train_loader, test_loader, model, loss_fn, optimizer, len(train_dataset))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Duration of training: {(end - start) / 60}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
